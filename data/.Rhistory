q()
fread(http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt)
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
structure(lrn14)
dim(lrn14)
View(lrn14)
the dimensions of the data
dim(lrn14)
# look at the structure of the data
str(lrn14)
# select the columns related to deep learning and create column 'deep' by averaging
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
# select the columns related to surface learning and create column 'surf' by averaging
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
# select the columns related to strategic learning and create column 'stra' by averaging
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
library(dplyr)
install.packages(dplyr)
.libPaths()
install.packages(dplyr)
install(dplyr)
mat <- read.csv("student-mat.csv", header = TRUE)
setwd("~/GitHub/IODS-project/data")
mat <- read.csv("student-mat.csv", header = TRUE)
str(mat)
mat
por <- read.csv("student-por.csv", header = TRUE)
str(mat)
str(por)
dim(mat)
dim(por)
mat <- read.csv("student-mat.csv", sep = ";", header = TRUE)
por <- read.csv("student-por.csv", sep= ";", header = TRUE)
str(mat)
str(por)
dim(mat)
dim(por)
join_by <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
math_por <- inner_join(math, por, by = join_by, suffix = c(".math", ".por"))
library(dplyr)
math_por <- inner_join(math, por, by = join_by, suffix = c(".math", ".por"))
mat_por <- inner_join(mat, por, by = join_by, suffix = c(".mat", ".por"))
notjoined_columns <- colnames(mat)[!colnames(mat) %in% join_by]
notjoined_columns
for(column_name in notjoined_columns) {
two_columns <- select(mat_por, starts_with(column_name))
first_column <- select(two_columns, 1)[[1]]
if(is.numeric(first_column)) {
alc[column_name] <- round(rowMeans(two_columns))
} else {
alc[column_name] <- first_column
}
}
alc <- select(mat_por, one_of(join_by))
notjoined_columns <- colnames(mat)[!colnames(mat) %in% join_by]
notjoined_columns
for(column_name in notjoined_columns) {
two_columns <- select(mat_por, starts_with(column_name))
first_column <- select(two_columns, 1)[[1]]
if(is.numeric(first_column)) {
alc[column_name] <- round(rowMeans(two_columns))
} else {
alc[column_name] <- first_column
}
}
str(mat_por)
str(alc)
alc <- mutate(alc, alc_use = (Dalc + Walc) / 2)
str(alc)
alc <- mutate(alc, high_use = alc_use > 2)
str(alc)
glimpse(alc)
?write.csv()
write.csv(alc, "alc.csv")
alc_test <-read.csv("alc.csv")
glimpse(alc_test)
write.csv(alc, "alc.csv", row.names = TRUE)
alc_test <-read.csv("alc.csv")
glimpse(alc_test)
write.csv(alc, "alc.csv", row.names = F)
alc_test<- read.csv("alc.csv")
glimpse(alc_test)
View(alc)
install("tidyr")
packages.install("tidyr")
install.packages("tidyr")
library(dplyr)
alc <- read.csv("data/alc.csv")
library(tidyr); library(dplyr); library(ggplot2)
variables <- c("high_use","failures","studytime","health","famsup")
alc_hyp <- select(alc, one_of(variables))
m <- glm(high_use ~ failures + health + famsup + studytime, data = alc_hyp, family = "binomial")
summary(m)
coef(m)
m <- glm(high_use ~ failures + health + famsup + studytime, data = alc_hyp, family = "binomial")
#summary
summary(m)
coef(m)
m <- glm(high_use ~ failures + health + famsup + studytime, data = alc_hyp, family = "binomial")
#summary
#summary(m)
#coef(m)
m <- glm(high_use ~ failures + health + famsup + studytime, data = alc_hyp, family = "binomial")
#summary
summary(m)
#coef(m)
m <- glm(high_use ~ failures + health + famsup + studytime, data = alc_hyp, family = "binomial")
#summary
summary(m)
OR <- coef(m) %>% exp
# compute confidence intervals (CI)
CI <- confint(m) %>% exp
# print out the odds ratios with their confidence intervals
cbind(OR, CI)
OR <- coef(m) %>% exp
# compute confidence intervals (CI)
CI <- confint(m) %>% exp
# print out the odds ratios with their confidence intervals
cbind(OR, CI)
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc_hyp$studytime, prob = alc$probability)
probabilities <- predict(m, type = "response")
alc <- mutate(alc_hyp, probability = probabilities)
# use the probabilities to make a prediction of high_use
alc <- mutate(alc_hyp, prediction = probabilities > 0.5)
# tabulate the target variable versus the predictions
table(high_use = alc_hyp$high_use, prediction = probabilities > 0.5)
probabilities <- predict(m, type = "response")
alc_hyp <- mutate(alc_hyp, probability = probabilities)
# use the probabilities to make a prediction of high_use
alc_hyp <- mutate(alc_hyp, prediction = probabilities > 0.5)
# tabulate the target variable versus the predictions
table(high_use = alc_hyp$high_use, prediction = probabilities > 0.5)
ggplot(alc_hyp$high_use, aes(high_use))
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc_hyp$high_use, prob = alc_hyp$probability)
install.packages("gridExtra")
